{"UID": "WOS:000694722900006", "static_data": {"summary": {"pub_info": {"coverdate": 2021, "journal_oas_gold": "N", "pubyear": 2021, "sortdate": "2021-01-01", "has_abstract": "Y", "pubtype": "Book", "page": {"end": 54, "begin": 45, "page_count": 10, "content": "45-54"}}, "names": {"count": 2, "name": [{"seq_no": 1, "role": "author", "full_name": "Gillis, Noa Baker", "claim_status": false, "last_name": "Gillis", "data-item-ids": {"data-item-id": {"type": "person", "content": "ILW-6337-2023", "id-type": "PreferredRID"}}, "display_name": "Gillis, Noa Baker", "wos_standard": "Gillis, NB", "r_id": "ILW-6337-2023", "daisng_id": 44526077, "first_name": "Noa Baker", "preferred_name": {"full_name": "Gillis, Noa Baker", "last_name": "Gillis", "first_name": "Noa Baker"}}, {"seq_no": 2, "role": "book_corp", "full_name": "Assoc Computat Linguist", "display_name": "Assoc Computat Linguist"}]}, "doctypes": {"doctype": "Proceedings Paper", "count": 1}, "publishers": {"publisher": {"names": {"count": 1, "name": {"seq_no": 1, "role": "publisher", "full_name": "ASSOC COMPUTATIONAL LINGUISTICS-ACL", "unified_name": "Assoc Computational Linguistics-Acl", "addr_no": 1, "display_name": "ASSOC COMPUTATIONAL LINGUISTICS-ACL"}}, "address_spec": {"city": "STROUDSBURG", "addr_no": 1, "full_address": "209 N EIGHTH STREET, STROUDSBURG, PA 18360 USA"}}}, "EWUID": {"WUID": {"coll_id": "WOS"}, "edition": [{"value": "WOS.ISTP"}, {"value": "WOS.ISSHP"}]}, "titles": {"count": 2, "title": [{"type": "source", "content": "GEBNLP 2021: THE 3RD WORKSHOP ON GENDER BIAS IN NATURAL LANGUAGE PROCESSING"}, {"type": "item", "content": "Sexism in the Judiciary: Bias Definition in NLP and in Our Courts"}]}, "conferences": {"conference": {"conf_dates": {"conf_date": {"conf_start": 20210805, "content": "AUG 05, 2021"}, "count": 1}, "conf_id": 344348, "conf_infos": {"count": 1, "conf_info": "3rd Workshop on Gender Bias in Natural Language Processing (GeBNLP), AUG05, 2021, ELECTR NETWORK"}, "conf_locations": {"count": 1, "conf_location": {"conf_state": "ELECTR NETWORK"}}, "conf_titles": {"count": 1, "conf_title": "3rd Workshop on Gender Bias in Natural Language Processing (GeBNLP)"}}, "count": 1}}, "item": {"book_pages": 111, "xsi:type": "itemType_wos", "coll_id": "WOS", "ids": {"avail": "N", "content": "BS1QN"}, "xmlns:xsi": "http://www.w3.org/2001/XMLSchema-instance", "bib_pagecount": {"type": "Book", "content": 111}, "book_notes": {"book_note": ["Figures", "Plates", "Color plates"], "count": 3}, "bib_id": ": 45-54 2021", "book_desc": {"bk_publisher": "ASSOC COMPUTATIONAL LINGUISTICS-ACL, 209 N EIGHTH STREET, STROUDSBURG, PA 18360 USA", "bk_binding": "P", "bk_prepay": "N"}}, "fullrecord_metadata": {"addresses": {"count": 0}, "category_info": {"subheadings": {"count": 1, "subheading": "Technology"}, "subjects": {"subject": [{"ascatype": "traditional", "code": "EP", "content": "Computer Science, Artificial Intelligence"}, {"ascatype": "traditional", "code": "EV", "content": "Computer Science, Interdisciplinary Applications"}, {"ascatype": "traditional", "code": "OT", "content": "Linguistics"}, {"ascatype": "extended", "content": "Computer Science"}, {"ascatype": "extended", "content": "Linguistics"}], "count": 5}, "headings": {"heading": ["Science & Technology", "Social Sciences"], "count": 2}}, "normalized_languages": {"count": 1, "language": {"type": "primary", "content": "English"}}, "languages": {"count": 1, "language": {"type": "primary", "content": "English"}}, "refs": {"count": 18}, "abstracts": {"count": 1, "abstract": {"abstract_text": {"p": "We analyze 6.7 million case law documents to determine the presence of gender bias within our judicial system. We find that current bias detection methods in NLP are insufficient to determine gender bias in our case law database and propose an alternative approach. We show that existing algorithms' inconsistent results are consequences of prior research's inconsistent definitions of biases themselves. Bias detection algorithms rely on groups of words to represent bias (e.g., `salary,' `job,' and `boss' to represent employment as a potentially biased theme against women in text). However, the methods to build these groups of words have several weaknesses, primarily that the word lists are based on the researchers' own intuitions. We suggest two new methods of automating the creation of word lists to represent biases. We find that our methods outperform current NLP bias detection methods. Our research improves the capabilities of NLP technology to detect bias and highlights gender biases present in influential case law. In order to test our NLP bias detection method's performance, we regress our results of bias in case law against U.S census data of women's participation in the workforce in the last 100 years.", "count": 1}}}, "normalized_doctypes": {"doctype": "Meeting", "count": 1}}}, "dates": {"date_loaded": "2021-10-02T23:59:59.00000", "date_modified": "2021-12-18T21:17:50.669021", "date_created": "2021-10-02T10:54:55.696494"}, "r_id_disclaimer": "ResearcherID data provided by Clarivate Analytics", "dynamic_data": {"citation_related": {"citation_topics": {"subj-group": {"subject": [{"content-type": "macro", "content-id": "6", "content": "Social Sciences"}, {"content-type": "meso", "content-id": "6.185", "content": "Communication"}, {"content-type": "micro", "content-id": "6.185.1644", "content": "Privacy"}]}}, "tc_list": {"silo_tc": {"coll_id": "WOS", "local_count": 0}}}, "cluster_related": {"identifiers": {"identifier": {"type": "eisbn", "value": "978-1-954085-61-9"}}}, "wos_usage": {"last180days": 0, "alltime": 2}}}